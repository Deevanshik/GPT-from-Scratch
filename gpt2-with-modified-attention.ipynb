{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2Tokenizer\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:20.938384Z","iopub.execute_input":"2025-04-20T06:13:20.938832Z","iopub.status.idle":"2025-04-20T06:13:30.062603Z","shell.execute_reply.started":"2025-04-20T06:13:20.938798Z","shell.execute_reply":"2025-04-20T06:13:30.061599Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Setting up device configurations\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device is: \", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:30.064945Z","iopub.execute_input":"2025-04-20T06:13:30.065425Z","iopub.status.idle":"2025-04-20T06:13:30.074922Z","shell.execute_reply.started":"2025-04-20T06:13:30.065399Z","shell.execute_reply":"2025-04-20T06:13:30.073922Z"}},"outputs":[{"name":"stdout","text":"Device is:  cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Creating Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntext = \"Every day is your\"\nencoded_input = tokenizer.encode(text)  # Returns a tensor\nprint(f\"Encoded input: {encoded_input}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:30.076453Z","iopub.execute_input":"2025-04-20T06:13:30.076838Z","iopub.status.idle":"2025-04-20T06:13:39.148417Z","shell.execute_reply.started":"2025-04-20T06:13:30.076804Z","shell.execute_reply":"2025-04-20T06:13:39.147351Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13322b65cdd04944b1eeb7e2fdd579ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb17c14424cc4554aba66c90a0fddeb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7599e716ad1a45b389cc4b91f339ee11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a46ed9678943eda85d7009c267d653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0180344ca1a463f9b0d4dcfa384fe15"}},"metadata":{}},{"name":"stdout","text":"Encoded input: [6109, 1110, 318, 534]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Embedding class\nclass Embedding(nn.Module):\n    def __init__(self, vocab_size, embed_size):\n        super().__init__()\n        self.embed_size = embed_size\n        self.embed = nn.Embedding(vocab_size, embed_size)\n\n    def forward(self, x):\n        # Create a mask for negative values\n        mask = (x < 0).unsqueeze(-1)  # shape: (B, T, 1)\n        \n        # Replace negative indices with 0 to avoid index error\n        x_clipped = x.clamp(min=0)\n\n        # Get embeddings and zero out positions where the original input was negative\n        emb = self.embed(x_clipped)  # (B, T, C)\n        emb = emb.masked_fill(mask, 0.0)\n        return emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.149287Z","iopub.execute_input":"2025-04-20T06:13:39.149738Z","iopub.status.idle":"2025-04-20T06:13:39.158025Z","shell.execute_reply.started":"2025-04-20T06:13:39.149705Z","shell.execute_reply":"2025-04-20T06:13:39.157009Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2. Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_size, max_seq_length=256):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(max_seq_length, embed_size)\n        \n    def forward(self, x):\n        T = x.shape[1]\n        pos_embed = self.pos_embedding(torch.arange(T, device=device)) # (T,C)\n        return x + pos_embed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.159018Z","iopub.execute_input":"2025-04-20T06:13:39.159335Z","iopub.status.idle":"2025-04-20T06:13:39.189309Z","shell.execute_reply.started":"2025-04-20T06:13:39.159313Z","shell.execute_reply":"2025-04-20T06:13:39.188283Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch.nn.functional as F\ndef generate_causal_mask(seq_length, proj_dim):\n    mask = torch.tril(torch.ones(seq_length, proj_dim))\n    return mask.unsqueeze(0).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.190430Z","iopub.execute_input":"2025-04-20T06:13:39.190836Z","iopub.status.idle":"2025-04-20T06:13:39.209150Z","shell.execute_reply.started":"2025-04-20T06:13:39.190806Z","shell.execute_reply":"2025-04-20T06:13:39.208209Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 3. Multi-Head Attention \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_size, num_heads, proj_dim, seq_len, sharing, proj, qkv_bias=False, layer_idx=None):\n        super().__init__()\n        self.embed_size = embed_size # -> d\n        self.num_heads = num_heads # -> h\n        self.head_dim = embed_size // num_heads # -> d/h\n        self.proj_dim = proj_dim  # -> k\n        self.seq_len = seq_len # -> N\n        self.sharing = sharing\n        self.proj = proj\n        self.layer_idx = layer_idx\n        \n        self.query = nn.Linear(embed_size, embed_size, bias=qkv_bias) # d*d\n        self.key = nn.Linear(embed_size, embed_size, bias=qkv_bias) # d*d\n        self.value = nn.Linear(embed_size, embed_size, bias=qkv_bias) # d*d\n        \n        self.out = nn.Linear(embed_size, embed_size) # fully connected FFNN from d -> d\n        \n    def forward(self, x, use_mask=False):\n        B, T, _ = x.shape # Batch size, Sequence Length, Embed size\n        h = self.num_heads\n        d_h = self.head_dim\n        N = self.seq_len\n        \n        q = self.query(x).view(B, -1, self.num_heads, self.head_dim).transpose(1, 2) # (B, N, d) â†’ (B, N, h, d/h) -> (B, h, N, d/h)\n        k = self.key(x).view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.value(x).view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        if(self.sharing == \"layerwise-sharing\"):\n            # layerwise sharing -> only 1 matrix throughout the model\n            projn_matrix = self.proj\n            k_proj = torch.einsum('bhnf,nk->bhkf', k, projn_matrix)\n            v_proj = torch.einsum('bhnf,nk->bhkf', v, projn_matrix)\n        elif(self.sharing == \"key-value-sharing\"):\n            # key-value sharing\n            projn_matrix = self.proj[self.layer_idx]\n            k_proj = torch.einsum('bhnf,nk->bhkf', k, projn_matrix)\n            v_proj = torch.einsum('bhnf,nk->bhkf', v, projn_matrix)\n        elif(self.sharing == \"headwise-sharing\"):\n            # headwise sharing\n            projn_matrix_E = self.proj[\"E\"][self.layer_idx]\n            projn_matrix_F = self.proj[\"F\"][self.layer_idx]\n            k_proj = torch.einsum('bhnf,nk->bhkf', k, projn_matrix_E)\n            v_proj = torch.einsum('bhnf,nk->bhkf', v, projn_matrix_F)\n            \n        attention = torch.matmul(q, k_proj.transpose(-1, -2)) / math.sqrt(self.head_dim) # (B, h, N, k)\n        if (use_mask):\n            mask = generate_causal_mask(T, self.proj_dim) # (1,1,N,k)\n            mask = mask.to(device)\n            attention = attention.masked_fill(mask == 0, float('-inf'))\n        attention = torch.softmax(attention, dim=-1)\n        \n        out = torch.matmul(attention, v_proj)\n        out = out.transpose(1, 2).contiguous().view(B, -1, self.embed_size)\n        return self.out(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.212419Z","iopub.execute_input":"2025-04-20T06:13:39.212761Z","iopub.status.idle":"2025-04-20T06:13:39.232909Z","shell.execute_reply.started":"2025-04-20T06:13:39.212727Z","shell.execute_reply":"2025-04-20T06:13:39.231802Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 5. Feed-Forward Network\nclass FeedForward(nn.Module):\n    def __init__(self, embed_size, ff_hidden_size):\n        super().__init__()\n        # First linear layer that transforms input from embedding size to hidden size\n        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n        # Second linear layer that transforms from hidden size back to embedding size\n        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n        # GELU activation function\n        self.gelu = nn.GELU()\n    def forward(self, x):\n        # Forward pass: apply the first linear layer, then GELU activation, and finally the second linear layer\n        return self.fc2(self.gelu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.234129Z","iopub.execute_input":"2025-04-20T06:13:39.234574Z","iopub.status.idle":"2025-04-20T06:13:39.258787Z","shell.execute_reply.started":"2025-04-20T06:13:39.234474Z","shell.execute_reply":"2025-04-20T06:13:39.257661Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Transformer Block\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_size, num_heads, ff_hidden_size, proj_dim, seq_len, sharing, proj, dropout=0.1, qkv_bias=False, layer_idx=None):\n        super().__init__()\n        # Layer norm before attention\n        self.ln1 = nn.LayerNorm(embed_size)\n        # Multi-Head Self-Attention\n        self.mha = MultiHeadAttention(embed_size, num_heads, proj_dim, seq_len, sharing, proj, qkv_bias=qkv_bias, layer_idx=layer_idx)\n        self.dropout1 = nn.Dropout(dropout)\n\n        # Layer norm before FFN\n        self.ln2 = nn.LayerNorm(embed_size)\n        self.ff = FeedForward(embed_size, ff_hidden_size)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, use_mask=False):\n        # MHA with Layer norm and residual connection\n        x = x + self.dropout1(self.mha(self.ln1(x), use_mask))\n        # FFN with layer norm and residual connection\n        x = x + self.dropout2(self.ff(self.ln2(x)))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.260096Z","iopub.execute_input":"2025-04-20T06:13:39.260460Z","iopub.status.idle":"2025-04-20T06:13:39.277684Z","shell.execute_reply.started":"2025-04-20T06:13:39.260434Z","shell.execute_reply":"2025-04-20T06:13:39.276532Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def projection(sharing, seq_length, proj_dim, n_layers=None):\n    if(sharing == \"layerwise-sharing\"):\n        proj = nn.Parameter(torch.empty(seq_length, proj_dim))\n        torch.nn.init.xavier_normal_(proj)\n        return proj\n    if(sharing == \"key-value-sharing\"):\n        proj = [nn.Parameter(torch.empty(seq_length, proj_dim)) for _ in range(n_layers)]\n        for proj_i in proj:\n            torch.nn.init.xavier_normal_(proj_i)\n        return proj\n    if(sharing == \"headwise-sharing\"):\n        E_proj = [nn.Parameter(torch.empty(seq_length, proj_dim)) for _ in range(n_layers)]\n        F_proj = [nn.Parameter(torch.empty(seq_length, proj_dim)) for _ in range(n_layers)]\n        for proj_i in E_proj:\n            torch.nn.init.xavier_normal_(proj_i)\n        for proj_i in F_proj:\n            torch.nn.init.xavier_normal_(proj_i)\n        proj_dict = {\n            \"E\" : E_proj,\n            \"F\" : F_proj\n        }\n        return proj_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.278871Z","iopub.execute_input":"2025-04-20T06:13:39.279246Z","iopub.status.idle":"2025-04-20T06:13:39.302701Z","shell.execute_reply.started":"2025-04-20T06:13:39.279214Z","shell.execute_reply":"2025-04-20T06:13:39.301770Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def right_pad_input(input_ids, context_length, pad_token_id=-1):\n    T = input_ids.shape\n    if T < context_length:\n        padding = torch.full((B, context_length - T), pad_token_id, dtype=input_ids.dtype, device=input_ids.device)\n        input_ids_padded = torch.cat([input_ids, padding], dim=1)\n    else:\n        input_ids_padded = input_ids[:, :context_length]\n    return input_ids_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.303699Z","iopub.execute_input":"2025-04-20T06:13:39.304095Z","iopub.status.idle":"2025-04-20T06:13:39.323934Z","shell.execute_reply.started":"2025-04-20T06:13:39.304012Z","shell.execute_reply":"2025-04-20T06:13:39.322797Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 7. GPT-2 Model\nclass GPT2_modified(nn.Module):\n    def __init__(self, config, seq_len):\n        super().__init__()\n        # Initializing embedding layer that converts token ids to embeddings\n        self.embedding = Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n        \n        # Initializing positional encoding \n        self.positional_encoding = PositionalEncoding(config[\"emb_dim\"], config[\"context_length\"])\n        \n        # Create a list of transformer blocks\n        if(config[\"sharing\"] == \"layerwise-sharing\"):\n            # Only 1 projection matrix\n            proj = projection(config[\"sharing\"], seq_len, config[\"proj_dim\"])\n            self.transformer_blocks = nn.ModuleList([\n                TransformerBlock(config[\"emb_dim\"], config[\"n_heads\"], config[\"emb_dim\"] * 4, config[\"proj_dim\"], seq_len, config[\"sharing\"], proj, config[\"drop_rate\"], config[\"qkv_bias\"])\n                for _ in range(config[\"n_layers\"])  # Repeat for num_layers \n            ])\n        \n        elif(config[\"sharing\"] == \"key-value-sharing\"):\n            # No. of projection matrices = config[\"n_layers\"]\n            proj = projection(config[\"sharing\"], seq_len, config[\"proj_dim\"], config[\"n_layers\"])\n            self.transformer_blocks = nn.ModuleList([\n                TransformerBlock(config[\"emb_dim\"], config[\"n_heads\"], config[\"emb_dim\"] * 4, config[\"proj_dim\"], seq_len, config[\"sharing\"], proj, config[\"drop_rate\"], config[\"qkv_bias\"], layer_idx)\n                for layer_idx in range(config[\"n_layers\"])  # Repeat for num_layers \n            ])\n            \n        elif(config[\"sharing\"] == \"headwise-sharing\"):\n            # No. of projection matrices = 2*config[\"n_layers\"]\n            proj = projection(config[\"sharing\"], seq_len, config[\"proj_dim\"], config[\"n_layers\"])\n            self.transformer_blocks = nn.ModuleList([\n                TransformerBlock(config[\"emb_dim\"], config[\"n_heads\"], config[\"emb_dim\"] * 4, config[\"proj_dim\"], seq_len, config[\"sharing\"], proj, config[\"drop_rate\"], config[\"qkv_bias\"], layer_idx)\n                for layer_idx in range(config[\"n_layers\"])  # Repeat for num_layers \n            ])\n\n        # Final linear layer to project the output back to the vocabulary size for logits\n        self.fc_out = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"])\n        \n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(config[\"drop_rate\"])\n        \n    def forward(self, x, use_mask=False, targets=None):\n        # Step 1: Convert input token IDs to embeddings and add positional encodings\n        x = self.dropout(self.positional_encoding(self.embedding(x)))\n        \n        # Step 2: Pass the embeddings through each transformer block\n        for block in self.transformer_blocks:\n            x = block(x, use_mask)  # Apply the transformer block with optional masking\n        \n        # Step 3: Calculate the logits\n        logits = self.fc_out(x)  # Shape: (batch_size, seq_length, vocab_size)\n\n        # Step 4: Calculating the loss\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, input_text, max_new_tokens, context_length, tokenizer, vocab_size, use_mask=False):\n        # input_text-> string array\n        x = tokenizer.encode(input_text) # list of length T, T is no. of tokens\n        x = torch.tensor(x).unsqueeze(0).to(device)\n        T = x.shape[1]\n        x = right_pad_input(x, context_length=context_length, pad_token_id = 0)\n        for _ in range(max_new_tokens):\n            # Get the predictions\n            logits, loss = self(x, use_mask) # (B, T, C)\n            \n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            \n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            \n            # sample from the distribution\n            x_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            \n            if(T < context_length):\n                x[0][T] = x_next[0][0]\n            else:\n                x = x[:, 1:]\n                x = torch.cat((x, x_next), dim=1)\n\n            input_text += tokenizer.decode(x_next[0][0])\n            T = T+1\n        return input_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:29.929999Z","iopub.execute_input":"2025-04-20T06:21:29.930470Z","iopub.status.idle":"2025-04-20T06:21:29.947689Z","shell.execute_reply.started":"2025-04-20T06:21:29.930437Z","shell.execute_reply":"2025-04-20T06:21:29.945855Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"tokenizer.decode(1232)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:32.280875Z","iopub.execute_input":"2025-04-20T06:21:32.281829Z","iopub.status.idle":"2025-04-20T06:21:32.288302Z","shell.execute_reply.started":"2025-04-20T06:21:32.281786Z","shell.execute_reply":"2025-04-20T06:21:32.287113Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"' leg'"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"GPT_CONFIG = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 256, # Context length\n    \"emb_dim\": 384,         # Embedding dimension\n    \"proj_dim\" : 96  ,     # Projected sequence length\n    \"n_heads\": 6,          # Number of attention heads\n    \"n_layers\": 6,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False,       # Query-Key-Value bias\n    \"use_mask\" : True,\n    \"batch_size\": 16,\n    \"num_epochs\": 20,\n    \"lr\": 10**-4,\n    \"sharing\": \"headwise-sharing\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:15:50.515369Z","iopub.execute_input":"2025-04-20T06:15:50.515777Z","iopub.status.idle":"2025-04-20T06:15:50.521225Z","shell.execute_reply.started":"2025-04-20T06:15:50.515752Z","shell.execute_reply":"2025-04-20T06:15:50.520034Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Downloading the Data -> Here we are using tiny shakespeare dataset from github\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:39.392161Z","iopub.execute_input":"2025-04-20T06:13:39.392483Z","iopub.status.idle":"2025-04-20T06:13:40.342513Z","shell.execute_reply.started":"2025-04-20T06:13:39.392453Z","shell.execute_reply":"2025-04-20T06:13:40.341299Z"}},"outputs":[{"name":"stdout","text":"--2025-04-20 06:13:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: â€˜input.txtâ€™\n\ninput.txt           100%[===================>]   1.06M  5.74MB/s    in 0.2s    \n\n2025-04-20 06:13:40 (5.74 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"with open('/kaggle/working/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(\"length of dataset in characters: \", len(text))\nprint(\"--------------------------------\")\nprint(text[:100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:40.343788Z","iopub.execute_input":"2025-04-20T06:13:40.344127Z","iopub.status.idle":"2025-04-20T06:13:40.351979Z","shell.execute_reply.started":"2025-04-20T06:13:40.344089Z","shell.execute_reply":"2025-04-20T06:13:40.351092Z"}},"outputs":[{"name":"stdout","text":"length of dataset in characters:  1115394\n--------------------------------\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"data = torch.tensor(tokenizer.encode(text), dtype = torch.int64)\nprint(data.shape, data.dtype)\nprint(data[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:40.352986Z","iopub.execute_input":"2025-04-20T06:13:40.353354Z","iopub.status.idle":"2025-04-20T06:13:42.983797Z","shell.execute_reply.started":"2025-04-20T06:13:40.353326Z","shell.execute_reply":"2025-04-20T06:13:42.982803Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([338025]) torch.int64\ntensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Train and Validation Split\nn = int(0.9*(len(data)))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:42.985320Z","iopub.execute_input":"2025-04-20T06:13:42.985704Z","iopub.status.idle":"2025-04-20T06:13:42.990789Z","shell.execute_reply.started":"2025-04-20T06:13:42.985669Z","shell.execute_reply":"2025-04-20T06:13:42.989836Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_batch(split, batch_size, seq_length): # Gives us batches on the bases of split\n    data = train_data if split == \"train\" else val_data\n    index = torch.randint(len(data) - seq_length, (batch_size,)) \n    # gives us list of random integers between range [0, len(data)-seq_length], where the number of lists is equal to batch size\n    x = torch.stack([data[i:i+seq_length] for i in index])\n    y = torch.stack([data[i+1:i+seq_length+1] for i in index])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:13:42.991793Z","iopub.execute_input":"2025-04-20T06:13:42.992162Z","iopub.status.idle":"2025-04-20T06:13:43.011225Z","shell.execute_reply.started":"2025-04-20T06:13:42.992138Z","shell.execute_reply":"2025-04-20T06:13:43.010114Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"m = GPT2_modified(GPT_CONFIG, GPT_CONFIG[\"context_length\"])\nmodel = m.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=GPT_CONFIG[\"lr\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:38.807794Z","iopub.execute_input":"2025-04-20T06:21:38.808164Z","iopub.status.idle":"2025-04-20T06:21:39.339051Z","shell.execute_reply.started":"2025-04-20T06:21:38.808139Z","shell.execute_reply":"2025-04-20T06:21:39.338021Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:39.340579Z","iopub.execute_input":"2025-04-20T06:21:39.340818Z","iopub.status.idle":"2025-04-20T06:21:39.347274Z","shell.execute_reply.started":"2025-04-20T06:21:39.340800Z","shell.execute_reply":"2025-04-20T06:21:39.346202Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"GPT2_modified(\n  (embedding): Embedding(\n    (embed): Embedding(50257, 384)\n  )\n  (positional_encoding): PositionalEncoding(\n    (pos_embedding): Embedding(256, 384)\n  )\n  (transformer_blocks): ModuleList(\n    (0-5): 6 x TransformerBlock(\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (mha): MultiHeadAttention(\n        (query): Linear(in_features=384, out_features=384, bias=False)\n        (key): Linear(in_features=384, out_features=384, bias=False)\n        (value): Linear(in_features=384, out_features=384, bias=False)\n        (out): Linear(in_features=384, out_features=384, bias=True)\n      )\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ff): FeedForward(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (gelu): GELU(approximate='none')\n      )\n      (dropout2): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc_out): Linear(in_features=384, out_features=50257, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:41.831154Z","iopub.execute_input":"2025-04-20T06:21:41.831486Z","iopub.status.idle":"2025-04-20T06:21:41.837825Z","shell.execute_reply.started":"2025-04-20T06:21:41.831464Z","shell.execute_reply":"2025-04-20T06:21:41.836628Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 49,385,809\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"num_tokens = len(train_data)\nbatches_per_epoch = num_tokens // (GPT_CONFIG[\"batch_size\"]*GPT_CONFIG[\"context_length\"])\n\nfor epoch in range(GPT_CONFIG[\"num_epochs\"]):\n    for step in range(batches_per_epoch):\n        xb, yb = get_batch('train', GPT_CONFIG[\"batch_size\"], GPT_CONFIG[\"context_length\"])\n        logits, loss = model(xb, GPT_CONFIG[\"use_mask\"], yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        if step % 50 == 0:\n            print(f\"Epoch {epoch+1} | Step {step+1}/{batches_per_epoch} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:26:33.712879Z","iopub.execute_input":"2025-04-20T06:26:33.713206Z","execution_failed":"2025-04-20T06:27:12.180Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Step 1/74 | Loss: 11.2932\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"input_text = \"hello! how are you?\"\nvocab_size = GPT_CONFIG[\"vocab_size\"]  # Assuming 50257\noutput = model.generate(input_text=input_text,\n               max_new_tokens=10, \n               context_length=GPT_CONFIG[\"context_length\"], \n               tokenizer=tokenizer,\n               vocab_size=vocab_size,\n               use_mask=True)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:21:44.491568Z","iopub.execute_input":"2025-04-20T06:21:44.491863Z","iopub.status.idle":"2025-04-20T06:21:46.285763Z","shell.execute_reply.started":"2025-04-20T06:21:44.491844Z","shell.execute_reply":"2025-04-20T06:21:46.284993Z"}},"outputs":[],"execution_count":44}]}